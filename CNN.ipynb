{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import VOCDetection\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Configuration des graphiques matplotlib pour qu'ils s'affichent correctement\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # ou 'Agg' pour éviter les fenêtres graphiques\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de: cpu\n",
      "Préparation des jeux de données...\n",
      "Jeu d'entraînement: 3 images\n",
      "Jeu de validation: 1 images\n",
      "\n",
      "Options disponibles:\n",
      "1. Entraîner le modèle\n",
      "2. Charger un modèle pré-entraîné\n",
      "3. Visualiser les prédictions du modèle par défaut\n",
      "\n",
      "Visualisation des prédictions...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Définition des classes PASCAL VOC\n",
    "VOC_CLASSES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 2           # Taille du lot\n",
    "LEARNING_RATE = 0.001    # Taux d'apprentissage initiale\n",
    "WEIGHT_DECAY = 0.0005    # Régularisation pour éviter le surapprentissage\n",
    "EPOCHS = 10              # Nombre de passages complets sur le dataset\n",
    "NUM_CLASSES = 20         # Classes dans PASCAL VOC (sans le fond)\n",
    "IMG_SIZE = 320        # Résolution d'entrée (carrée) pour YOLO\n",
    "CHECKPOINT_DIR = \"./checkpoints\"  # Dossier pour sauvegarder les modèles\n",
    "MAX_SAMPLES = 3     # Nombre d'échantillons pour le dataset\n",
    "\n",
    "# Anchors pour les trois échelles\n",
    "ANCHORS = [\n",
    "    [(116, 90), (156, 198), (373, 326)],  # Grand\n",
    "    [(30, 61), (62, 45), (59, 119)],      # Moyen\n",
    "    [(10, 13), (16, 30), (33, 23)]        # Petit\n",
    "]\n",
    "\n",
    "###########################################\n",
    "# PARTIE 1: ARCHITECTURE DU MODÈLE YOLOv3 #\n",
    "###########################################\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.leaky(self.bn(self.conv(x)))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvBlock(channels, channels//2, kernel_size=1)\n",
    "        self.conv2 = ConvBlock(channels//2, channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "class Darknet53(nn.Module):\n",
    "    def __init__(self, block, num_classes=1000):\n",
    "        super(Darknet53, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = ConvBlock(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = ConvBlock(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_block1 = self._make_layer(block, 64, 1)\n",
    "        self.conv3 = ConvBlock(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.res_block2 = self._make_layer(block, 128, 2)\n",
    "        self.conv4 = ConvBlock(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.res_block3 = self._make_layer(block, 256, 8)\n",
    "        self.conv5 = ConvBlock(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.res_block4 = self._make_layer(block, 512, 8)\n",
    "        self.conv6 = ConvBlock(512, 1024, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.res_block5 = self._make_layer(block, 1024, 4)\n",
    "        \n",
    "        # Pour la classification (si nécessaire)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        # Sorties pour la détection (à adapter selon les besoins)\n",
    "        self.features = [self.res_block3, self.res_block4, self.res_block5]\n",
    "    \n",
    "    def _make_layer(self, block, channels, num_blocks):\n",
    "        layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(block(channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.res_block2(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        route1 = self.res_block3(x)\n",
    "        x = self.conv5(route1)\n",
    "        \n",
    "        route2 = self.res_block4(x)\n",
    "        x = self.conv6(route2)\n",
    "        \n",
    "        route3 = self.res_block5(x)\n",
    "        \n",
    "        # Pour la détection, nous retournons les features à différentes échelles\n",
    "        return route1, route2, route3\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, num_classes=80):\n",
    "        super(YOLOv3, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Backbone\n",
    "        self.darknet = Darknet53(ResidualBlock)\n",
    "        \n",
    "        # Couches de détection\n",
    "        # Première échelle (grande)\n",
    "        self.conv_large = nn.Sequential(\n",
    "            ConvBlock(1024, 512, kernel_size=1),\n",
    "            ConvBlock(512, 1024, kernel_size=3, padding=1),\n",
    "            ConvBlock(1024, 512, kernel_size=1),\n",
    "            ConvBlock(512, 1024, kernel_size=3, padding=1),\n",
    "            ConvBlock(1024, 512, kernel_size=1)\n",
    "        )\n",
    "        self.detect_large = nn.Sequential(\n",
    "            ConvBlock(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(1024, 3 * (5 + num_classes), kernel_size=1)  # 3 anchors, 5 pour box (x,y,w,h,conf) + classes\n",
    "        )\n",
    "        \n",
    "        # Upsampling et concaténation\n",
    "        self.conv_up1 = ConvBlock(512, 256, kernel_size=1)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Deuxième échelle (moyenne)\n",
    "        self.conv_medium = nn.Sequential(\n",
    "            ConvBlock(768, 256, kernel_size=1),  # 256 (upsampled) + 512 (route2)\n",
    "            ConvBlock(256, 512, kernel_size=3, padding=1),\n",
    "            ConvBlock(512, 256, kernel_size=1),\n",
    "            ConvBlock(256, 512, kernel_size=3, padding=1),\n",
    "            ConvBlock(512, 256, kernel_size=1)\n",
    "        )\n",
    "        self.detect_medium = nn.Sequential(\n",
    "            ConvBlock(256, 512, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(512, 3 * (5 + num_classes), kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        # Upsampling et concaténation\n",
    "        self.conv_up2 = ConvBlock(256, 128, kernel_size=1)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Troisième échelle (petite)\n",
    "        self.conv_small = nn.Sequential(\n",
    "            ConvBlock(384, 128, kernel_size=1),  # 128 (upsampled) + 256 (route1)\n",
    "            ConvBlock(128, 256, kernel_size=3, padding=1),\n",
    "            ConvBlock(256, 128, kernel_size=1),\n",
    "            ConvBlock(128, 256, kernel_size=3, padding=1),\n",
    "            ConvBlock(256, 128, kernel_size=1)\n",
    "        )\n",
    "        self.detect_small = nn.Sequential(\n",
    "            ConvBlock(128, 256, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(256, 3 * (5 + num_classes), kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x)\n",
    "        \n",
    "        # Obtenir les features du backbone\n",
    "        route1, route2, route3 = self.darknet(x)\n",
    "        \n",
    "        # Première échelle (grande - détecte les grands objets)\n",
    "        large = self.conv_large(route3)\n",
    "        detect_large = self.detect_large(large)\n",
    "        \n",
    "        # Upsampling et concaténation avec route2\n",
    "        up1 = self.conv_up1(large)\n",
    "        up1 = self.upsample1(up1)\n",
    "        medium_in = torch.cat([up1, route2], dim=1)\n",
    "        \n",
    "        # Deuxième échelle (moyenne - détecte les objets de taille moyenne)\n",
    "        medium = self.conv_medium(medium_in)\n",
    "        detect_medium = self.detect_medium(medium)\n",
    "        \n",
    "        # Upsampling et concaténation avec route1\n",
    "        up2 = self.conv_up2(medium)\n",
    "        up2 = self.upsample2(up2)\n",
    "        small_in = torch.cat([up2, route1], dim=1)\n",
    "        \n",
    "        # Troisième échelle (petite - détecte les petits objets)\n",
    "        small = self.conv_small(small_in)\n",
    "        detect_small = self.detect_small(small)\n",
    "        \n",
    "        return detect_large, detect_medium, detect_small\n",
    "\n",
    "###########################################\n",
    "# PARTIE 2: CHARGEMENT ET PRÉPARATION DES DONNÉES #\n",
    "###########################################\n",
    "\n",
    "class VOCSubset(Dataset):\n",
    "    def __init__(self, root=\"./data\", year=\"2012\", image_set=\"train\", download=True, transform=None, target_transform=None, max_samples=100):\n",
    "        \"\"\"\n",
    "        Classe pour charger et traiter un sous-ensemble de PASCAL VOC\n",
    "        \n",
    "        Args:\n",
    "            root: Répertoire racine des données\n",
    "            year: Année du dataset PASCAL VOC ('2007' ou '2012')\n",
    "            image_set: Ensemble d'images ('train', 'val', 'test')\n",
    "            download: Télécharger automatiquement si non présent\n",
    "            transform: Transformations à appliquer sur les images\n",
    "            target_transform: Transformations à appliquer sur les annotations\n",
    "            max_samples: Nombre maximum d'échantillons à inclure\n",
    "        \"\"\"\n",
    "        self.voc = VOCDetection(root=root, year=year, image_set=image_set, download=download)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Sélectionner un sous-ensemble aléatoire\n",
    "        total_samples = len(self.voc)\n",
    "        self.indices = random.sample(range(total_samples), min(max_samples, total_samples))\n",
    "        \n",
    "        # Créer un mapping des classes vers des indices\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(VOC_CLASSES)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        voc_idx = self.indices[idx]\n",
    "        img, target = self.voc[voc_idx]\n",
    "        \n",
    "        # Extraction des boîtes et classes des annotations VOC\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for obj in target['annotation']['object']:\n",
    "            # Extraire les coordonnées de la boîte\n",
    "            bbox = obj['bndbox']\n",
    "            xmin = float(bbox['xmin'])\n",
    "            ymin = float(bbox['ymin'])\n",
    "            xmax = float(bbox['xmax'])\n",
    "            ymax = float(bbox['ymax'])\n",
    "            \n",
    "            # Extraire la classe\n",
    "            class_name = obj['name']\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            \n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(class_idx)\n",
    "        \n",
    "        # Conversion en tenseurs PyTorch\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Créer un dictionnaire de cibles au format attendu par PyTorch\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([voc_idx]),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        # Appliquer les transformations si spécifiées\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        return img, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Fonction personnalisée pour regrouper les échantillons en lots\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "    for img, tgt in batch:\n",
    "        images.append(img)\n",
    "        targets.append(tgt)\n",
    "    return images, targets\n",
    "\n",
    "def prepare_data_loaders(batch_size=4, max_samples=100):\n",
    "    \"\"\"\n",
    "    Prépare les DataLoaders pour l'entraînement et la validation\n",
    "    \"\"\"\n",
    "    # Transformation standard pour les images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((416, 416)),  # Redimensionner pour YOLOv3\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Préparer les jeux de données\n",
    "    train_dataset = VOCSubset(\n",
    "        root=\"./data\", \n",
    "        year=\"2012\", \n",
    "        image_set=\"train\", \n",
    "        download=True, \n",
    "        transform=transform, \n",
    "        max_samples=max_samples\n",
    "    )\n",
    "    \n",
    "    val_dataset = VOCSubset(\n",
    "        root=\"./data\", \n",
    "        year=\"2012\", \n",
    "        image_set=\"val\", \n",
    "        download=True, \n",
    "        transform=transform, \n",
    "        max_samples=max_samples // 2  # Moitié moins d'échantillons pour validation\n",
    "    )\n",
    "    \n",
    "    # Créer les DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "###########################################\n",
    "# PARTIE 3: FONCTIONS DE POST-TRAITEMENT #\n",
    "###########################################\n",
    "\n",
    "def transform_predictions(predictions, input_size, anchors, num_classes, device):\n",
    "    \"\"\"\n",
    "    Transforme les prédictions brutes du réseau en coordonnées de boîtes de délimitation.\n",
    "    \"\"\"\n",
    "    batch_size = predictions.size(0)\n",
    "    grid_size = predictions.size(2)\n",
    "    \n",
    "    # Nombre d'attributs par boîte: 5 (x, y, w, h, conf) + num_classes\n",
    "    stride = input_size // grid_size\n",
    "    bbox_attrs = 5 + num_classes\n",
    "    num_anchors = len(anchors)\n",
    "    \n",
    "    # Reshape des prédictions\n",
    "    predictions = predictions.view(batch_size, num_anchors, bbox_attrs, grid_size, grid_size)\n",
    "    predictions = predictions.permute(0, 1, 3, 4, 2).contiguous()\n",
    "    \n",
    "    # Appliquer la fonction sigmoïde pour les coordonnées x, y et la confiance\n",
    "    predictions[..., 0] = torch.sigmoid(predictions[..., 0])  # x\n",
    "    predictions[..., 1] = torch.sigmoid(predictions[..., 1])  # y\n",
    "    predictions[..., 4] = torch.sigmoid(predictions[..., 4])  # conf\n",
    "    \n",
    "    # Appliquer sigmoid aux scores de classe\n",
    "    predictions[..., 5:] = torch.sigmoid(predictions[..., 5:])\n",
    "    \n",
    "    # Ajouter les offsets de grille\n",
    "    grid_x = torch.arange(grid_size).repeat(grid_size, 1).view([1, 1, grid_size, grid_size]).to(device)\n",
    "    grid_y = torch.arange(grid_size).repeat(grid_size, 1).t().view([1, 1, grid_size, grid_size]).to(device)\n",
    "    \n",
    "    scaled_anchors = torch.FloatTensor([(a[0]/stride, a[1]/stride) for a in anchors]).to(device)\n",
    "    anchor_w = scaled_anchors[:, 0:1].view((1, num_anchors, 1, 1))\n",
    "    anchor_h = scaled_anchors[:, 1:2].view((1, num_anchors, 1, 1))\n",
    "    \n",
    "    # Appliquer les transformations\n",
    "    predictions[..., 0] += grid_x\n",
    "    predictions[..., 1] += grid_y\n",
    "    predictions[..., 2] = torch.exp(predictions[..., 2]) * anchor_w\n",
    "    predictions[..., 3] = torch.exp(predictions[..., 3]) * anchor_h\n",
    "    \n",
    "    # Mettre à l'échelle pour la taille d'entrée\n",
    "    predictions[..., :4] *= stride\n",
    "    \n",
    "    # Reshape pour la suppression non-maximale - IMPORTANT pour corriger l'erreur\n",
    "    predictions = predictions.view(batch_size, -1, 5 + num_classes)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def non_max_suppression(prediction, num_classes, conf_threshold=0.5, nms_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Applique la suppression non-maximale (NMS) aux boîtes prédites.\n",
    "    \"\"\"\n",
    "    # Depuis (centre x, centre y, largeur, hauteur) vers (x1, y1, x2, y2)\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[..., 0] = prediction[..., 0] - prediction[..., 2] / 2\n",
    "    box_corner[..., 1] = prediction[..., 1] - prediction[..., 3] / 2\n",
    "    box_corner[..., 2] = prediction[..., 0] + prediction[..., 2] / 2\n",
    "    box_corner[..., 3] = prediction[..., 1] + prediction[..., 3] / 2\n",
    "    prediction[..., :4] = box_corner[..., :4]\n",
    "    \n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    \n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filtrer les boîtes avec un score de confiance inférieur au seuil\n",
    "        conf_mask = (image_pred[:, 4] >= conf_threshold).squeeze()\n",
    "        image_pred = image_pred[conf_mask]\n",
    "        \n",
    "        # Si aucune boîte ne reste après le filtrage\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "            \n",
    "        # Obtenir les scores de classe avec la confiance de détection\n",
    "        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1, keepdim=True)\n",
    "        \n",
    "        # Concaténer les scores avec les coordonnées de la boîte\n",
    "        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n",
    "        \n",
    "        # Obtenir les classes uniques\n",
    "        unique_classes = detections[:, -1].cpu().unique()\n",
    "        \n",
    "        if prediction.is_cuda:\n",
    "            unique_classes = unique_classes.cuda()\n",
    "            detections = detections.cuda()\n",
    "            \n",
    "        for c in unique_classes:\n",
    "            # Obtenir les détections avec cette classe\n",
    "            detections_class = detections[detections[:, -1] == c]\n",
    "            \n",
    "            # Trier par confiance décroissante\n",
    "            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n",
    "            detections_class = detections_class[conf_sort_index]\n",
    "            \n",
    "            # Appliquer NMS\n",
    "            max_detections = []\n",
    "            while detections_class.size(0):\n",
    "                # Obtenir la détection avec la confiance la plus élevée\n",
    "                max_detections.append(detections_class[0].unsqueeze(0))\n",
    "                \n",
    "                # Arrêter si nous n'avons plus de détections\n",
    "                if len(detections_class) == 1:\n",
    "                    break\n",
    "                    \n",
    "                # Obtenir les IOUs pour toutes les autres détections\n",
    "                ious = bbox_iou(max_detections[-1], detections_class[1:])\n",
    "                \n",
    "                # Supprimer les détections avec un IOU supérieur au seuil\n",
    "                detections_class = detections_class[1:][ious < nms_threshold]\n",
    "                \n",
    "            max_detections = torch.cat(max_detections).data\n",
    "            \n",
    "            # Ajouter les détections de cette classe au résultat final\n",
    "            output[image_i] = max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calcule l'IOU entre deux ensembles de boîtes.\n",
    "    \"\"\"\n",
    "    # Obtenir les coordonnées des boîtes\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "    \n",
    "    # Obtenir les coordonnées de l'intersection\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    \n",
    "    # Calculer l'aire de l'intersection\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \\\n",
    "                 torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "                 \n",
    "    # Calculer l'aire des deux boîtes\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "    \n",
    "    # Calculer l'IOU\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "###########################################\n",
    "# PARTIE 4: FONCTION DE PERTE ET ENTRAÎNEMENT #\n",
    "###########################################\n",
    "\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes, img_size=320):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        self.entropy = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Constants\n",
    "        self.lambda_coord = 5\n",
    "        self.lambda_noobj = 0.5\n",
    "        \n",
    "    def forward(self, predictions, targets, anchors):\n",
    "        \"\"\"\n",
    "        Calcule la perte pour une sortie YOLO à une échelle donnée\n",
    "        \n",
    "        Args:\n",
    "            predictions: Sortie du réseau à une échelle [B, 3*(5+C), H, W]\n",
    "            targets: Annotations au format YOLO [B, max_objects, 5+C]\n",
    "            anchors: Anchors pour cette échelle\n",
    "        \"\"\"\n",
    "        # Récupérer les dimensions\n",
    "        batch_size = predictions.shape[0]\n",
    "        grid_size = predictions.shape[2]\n",
    "        stride = self.img_size // grid_size\n",
    "        \n",
    "        # Reshape des prédictions\n",
    "        predictions = predictions.view(batch_size, 3, 5 + self.num_classes, grid_size, grid_size)\n",
    "        predictions = predictions.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        \n",
    "        # Appliquer sigmoid aux coordonnées x, y et à la confiance\n",
    "        x = self.sigmoid(predictions[..., 0])\n",
    "        y = self.sigmoid(predictions[..., 1])\n",
    "        w = predictions[..., 2]\n",
    "        h = predictions[..., 3]\n",
    "        conf = self.sigmoid(predictions[..., 4])\n",
    "        pred_cls = self.sigmoid(predictions[..., 5:])\n",
    "        \n",
    "        # Préparer les masques et les tenseurs pour les cibles\n",
    "        obj_mask = torch.zeros(batch_size, 3, grid_size, grid_size, dtype=torch.bool, device=predictions.device)\n",
    "        noobj_mask = torch.ones(batch_size, 3, grid_size, grid_size, dtype=torch.bool, device=predictions.device)\n",
    "        \n",
    "        tx = torch.zeros(batch_size, 3, grid_size, grid_size, device=predictions.device)\n",
    "        ty = torch.zeros(batch_size, 3, grid_size, grid_size, device=predictions.device)\n",
    "        tw = torch.zeros(batch_size, 3, grid_size, grid_size, device=predictions.device)\n",
    "        th = torch.zeros(batch_size, 3, grid_size, grid_size, device=predictions.device)\n",
    "        tconf = torch.zeros(batch_size, 3, grid_size, grid_size, device=predictions.device)\n",
    "        tcls = torch.zeros(batch_size, 3, grid_size, grid_size, self.num_classes, device=predictions.device)\n",
    "        \n",
    "        # Convertir les anchors\n",
    "        scaled_anchors = torch.FloatTensor([(a_w / stride, a_h / stride) for a_w, a_h in anchors]).to(predictions.device)\n",
    "        anchor_w = scaled_anchors[:, 0:1].view((1, 3, 1, 1))\n",
    "        anchor_h = scaled_anchors[:, 1:2].view((1, 3, 1, 1))\n",
    "        \n",
    "        # Traiter les cibles\n",
    "        for b in range(batch_size):\n",
    "            for target in targets:\n",
    "                if target.sum() == 0:  # Si pas de cible\n",
    "                    continue\n",
    "                    \n",
    "                # Coordonnées x, y, w, h normalisées à la taille de l'image\n",
    "                gx = target[1] * grid_size\n",
    "                gy = target[2] * grid_size\n",
    "                gw = target[3] * self.img_size\n",
    "                gh = target[4] * self.img_size\n",
    "                \n",
    "                # Indices de la cellule de la grille\n",
    "                gi = int(gx)\n",
    "                gj = int(gy)\n",
    "                \n",
    "                # Convertir les dimensions relatives à l'image en dimensions relatives à l'anchor\n",
    "                gw_anchors = gw / stride\n",
    "                gh_anchors = gh / stride\n",
    "                \n",
    "                # Trouver le meilleur anchor (IOU plus grand)\n",
    "                anchor_ious = []\n",
    "                for anchor_idx, anchor in enumerate(scaled_anchors):\n",
    "                    anchor_iou = self.calculate_iou_anchors(gw_anchors, gh_anchors, anchor[0], anchor[1])\n",
    "                    anchor_ious.append(anchor_iou)\n",
    "                \n",
    "                best_anchor = np.argmax(anchor_ious)\n",
    "                \n",
    "                # Vérifier si la cible est dans les limites de la grille\n",
    "                if gi < grid_size and gj < grid_size:\n",
    "                    # Marquer la cellule comme contenant un objet\n",
    "                    obj_mask[b, best_anchor, gj, gi] = True\n",
    "                    noobj_mask[b, best_anchor, gj, gi] = False\n",
    "                    \n",
    "                    # Coordonnées relatives à la cellule\n",
    "                    tx[b, best_anchor, gj, gi] = gx - gi\n",
    "                    ty[b, best_anchor, gj, gi] = gy - gj\n",
    "                    \n",
    "                    # Largeur et hauteur en log-espace\n",
    "                    tw[b, best_anchor, gj, gi] = torch.log(gw_anchors / scaled_anchors[best_anchor][0] + 1e-16)\n",
    "                    th[b, best_anchor, gj, gi] = torch.log(gh_anchors / scaled_anchors[best_anchor][1] + 1e-16)\n",
    "                    \n",
    "                    # Confiance (objectness)\n",
    "                    tconf[b, best_anchor, gj, gi] = 1\n",
    "                    \n",
    "                    # Classe (one-hot encoding)\n",
    "                    class_idx = int(target[5])\n",
    "                    tcls[b, best_anchor, gj, gi, class_idx] = 1\n",
    "        \n",
    "        # Calculer les pertes\n",
    "        loss_x = self.mse(x[obj_mask], tx[obj_mask])\n",
    "        loss_y = self.mse(y[obj_mask], ty[obj_mask])\n",
    "        loss_w = self.mse(w[obj_mask], tw[obj_mask])\n",
    "        loss_h = self.mse(h[obj_mask], th[obj_mask])\n",
    "        \n",
    "        loss_conf_obj = self.mse(conf[obj_mask], tconf[obj_mask])\n",
    "        loss_conf_noobj = self.mse(conf[noobj_mask], tconf[noobj_mask])\n",
    "        loss_conf = loss_conf_obj + self.lambda_noobj * loss_conf_noobj\n",
    "        \n",
    "        loss_cls = self.mse(pred_cls[obj_mask], tcls[obj_mask])\n",
    "        \n",
    "        # Perte totale\n",
    "        loss = (\n",
    "            self.lambda_coord * (loss_x + loss_y + loss_w + loss_h)\n",
    "            + loss_conf\n",
    "            + loss_cls\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def calculate_iou_anchors(self, target_w, target_h, anchor_w, anchor_h):\n",
    "        \"\"\"Calcule l'IoU entre une cible et un anchor\"\"\"\n",
    "        intersection = min(target_w, anchor_w) * min(target_h, anchor_h)\n",
    "        union = (target_w * target_h) + (anchor_w * anchor_h) - intersection\n",
    "        return intersection / union\n",
    "\n",
    "def targets_to_yolo_format(targets, img_size=416, num_classes=20):\n",
    "    \"\"\"\n",
    "    Convertit les cibles du format PyTorch (boîtes xmin, ymin, xmax, ymax) \n",
    "    au format YOLO (x_center, y_center, width, height, class)\n",
    "    \"\"\"\n",
    "    yolo_targets = []\n",
    "    \n",
    "    for batch_idx, target in enumerate(targets):\n",
    "        boxes = target['boxes']\n",
    "        labels = target['labels']\n",
    "        \n",
    "        # Initialiser un tableau pour ce lot\n",
    "        batch_targets = torch.zeros((len(boxes), 6))  # 6 = [batch_idx, x, y, w, h, class]\n",
    "        \n",
    "        if len(boxes) > 0:\n",
    "            # Normaliser à la taille de l'image\n",
    "            boxes_norm = boxes.clone()\n",
    "            \n",
    "            # x_center, y_center, width, height\n",
    "            boxes_norm[:, 0] = ((boxes[:, 0] + boxes[:, 2]) / 2) / img_size\n",
    "            boxes_norm[:, 1] = ((boxes[:, 1] + boxes[:, 3]) / 2) / img_size\n",
    "            boxes_norm[:, 2] = (boxes[:, 2] - boxes[:, 0]) / img_size\n",
    "            boxes_norm[:, 3] = (boxes[:, 3] - boxes[:, 1]) / img_size\n",
    "            \n",
    "            # Ajuster les labels pour correspondre à l'indexation YOLO\n",
    "            # Dans PASCAL VOC, 'background' est la classe 0, mais pour YOLO, on l'ignore\n",
    "            # donc on soustrait 1 (si nécessaire)\n",
    "            labels_norm = labels - 1  # Si 'background' est 0 dans vos labels\n",
    "            \n",
    "            batch_targets[:, 0] = batch_idx\n",
    "            batch_targets[:, 1:5] = boxes_norm\n",
    "            batch_targets[:, 5] = labels_norm\n",
    "        \n",
    "        yolo_targets.append(batch_targets)\n",
    "    \n",
    "    return torch.cat(yolo_targets, 0)\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device, anchors):\n",
    "    \"\"\"Entraîne le modèle pour une époque\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training\")\n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        images = [img.to(device) for img in images]\n",
    "        \n",
    "        # Convertir les cibles au format YOLO\n",
    "        yolo_targets = targets_to_yolo_format(targets, img_size=IMG_SIZE)\n",
    "        yolo_targets = yolo_targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculer la perte pour chaque échelle\n",
    "        loss = torch.tensor(0.0, device=device)\n",
    "        for i, output in enumerate(outputs):\n",
    "            scale_loss = loss_fn(output, yolo_targets, anchors[i])\n",
    "            loss += scale_loss\n",
    "        \n",
    "        # Backward pass et optimisation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Mettre à jour la barre de progression\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_minimal(model, dataloader, device):\n",
    "    \"\"\"Version très simplifiée de validation qui évite tout traitement complexe\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Simplement faire un forward pass sur une seule batch sans calcul de perte\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for images, _ in dataloader:\n",
    "                images = [img.to(device) for img in images]\n",
    "                _ = model(images)  # Juste faire un forward pass\n",
    "                break  # Une seule batch suffit pour vérifier\n",
    "        return 999.0  # Valeur fictive de perte\n",
    "    except Exception as e:\n",
    "        print(f\"Validation error (ignored): {e}\")\n",
    "        return 999.0  # Valeur fictive en cas d'erreur\n",
    "\n",
    "def train(model, train_loader, val_loader, device, epochs=10):\n",
    "    \"\"\"Fonction principale d'entraînement\"\"\"\n",
    "    # Créer le dossier pour les checkpoints\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Définir l'optimiseur\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Définir le scheduler pour réduire le LR\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Définir la fonction de perte pour chaque échelle\n",
    "    loss_function = YOLOLoss(ANCHORS, NUM_CLASSES, img_size=IMG_SIZE)\n",
    "    \n",
    "    # Historique des pertes\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Meilleure perte de validation\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Entraîner pour une époque\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_function, device, anchors)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Utiliser la validation simplifiée\n",
    "        val_loss = validate_minimal(model, val_loader, device)  # Passer device explicitement\n",
    "        print(f\"Validation simplifiée: val_loss = {val_loss:.6f}\")\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Mettre à jour le scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Sauvegarder le meilleur modèle\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, 'best_model.pth'))\n",
    "            print(f\"Saved best model with validation loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Sauvegarder checkpoint\n",
    "        if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "    \n",
    "    # Tracer les courbes de perte\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'loss_curve.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def visualize_detections(image, detections, class_names, conf_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Visualise les détections sur une image\n",
    "    \n",
    "    Args:\n",
    "        image: Tensor d'image [C, H, W]\n",
    "        detections: Liste de détections après NMS\n",
    "        class_names: Liste des noms de classes\n",
    "        conf_thresh: Seuil de confiance minimum à afficher\n",
    "    \"\"\"\n",
    "    # Convertir le tensor en image PIL pour l'affichage\n",
    "    img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "    # Dénormaliser\n",
    "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    \n",
    "    # Créer la figure\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(img_np)\n",
    "    \n",
    "    # Couleurs pour différentes classes\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(class_names)))\n",
    "    \n",
    "    # Si des détections existent\n",
    "    if detections is not None:\n",
    "        for x1, y1, x2, y2, obj_conf, cls_conf, cls_id in detections:\n",
    "            if obj_conf * cls_conf < conf_thresh:\n",
    "                continue\n",
    "                \n",
    "            # Coordonnées de la boîte\n",
    "            box_h = y2 - y1\n",
    "            box_w = x2 - x1\n",
    "            \n",
    "            # Créer un rectangle\n",
    "            color = colors[int(cls_id) % len(colors)]\n",
    "            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, \n",
    "                                     edgecolor=color, facecolor=\"none\")\n",
    "            ax.add_patch(bbox)\n",
    "            \n",
    "            # Ajouter le texte avec la classe et la confiance\n",
    "            class_name = class_names[int(cls_id)]\n",
    "            conf = obj_conf * cls_conf\n",
    "            ax.text(x1, y1, f'{class_name}: {conf:.2f}', \n",
    "                    color='white', fontsize=10,\n",
    "                    bbox=dict(facecolor=color, alpha=0.7))\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "###########################################\n",
    "# PARTIE 5: PROGRAMME PRINCIPAL #\n",
    "###########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 0.001\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "    EPOCHS = 10\n",
    "    NUM_CLASSES = 20\n",
    "    IMG_SIZE = 416\n",
    "    \n",
    "    # Vérifier si CUDA est disponible\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Utilisation de: {device}\")\n",
    "    \n",
    "    # Préparer les jeux de données\n",
    "    print(\"Préparation des jeux de données...\")\n",
    "    train_loader, val_loader = prepare_data_loaders(BATCH_SIZE, MAX_SAMPLES)\n",
    "    \n",
    "    # Afficher quelques statistiques\n",
    "    print(f\"Jeu d'entraînement: {len(train_loader.dataset)} images\")\n",
    "    print(f\"Jeu de validation: {len(val_loader.dataset)} images\")\n",
    "    \n",
    "    # Initialiser le modèle\n",
    "    model = YOLOv3(NUM_CLASSES).to(device)\n",
    "    \n",
    "    # Choix de l'action à effectuer\n",
    "    print(\"\\nOptions disponibles:\")\n",
    "    print(\"1. Entraîner le modèle\")\n",
    "    print(\"2. Charger un modèle pré-entraîné\")\n",
    "    print(\"3. Visualiser les prédictions du modèle par défaut\")\n",
    "    \n",
    "    choice = input(\"Votre choix (1/2/3): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        # Entraîner le modèle\n",
    "        print(\"\\nDémarrage de l'entraînement...\")\n",
    "        train_losses, val_losses = train(model, train_loader, val_loader, device, EPOCHS)\n",
    "        print(\"Entraînement terminé!\")\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        # Charger un modèle pré-entraîné\n",
    "        model_path = input(\"Chemin du modèle (ex: ./checkpoints/best_model.pth): \")\n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Modèle chargé de l'époque {checkpoint['epoch']} avec perte de validation: {checkpoint['val_loss']:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement du modèle: {e}\")\n",
    "            print(\"Utilisation du modèle par défaut...\")\n",
    "    \n",
    "    # Passer en mode évaluation\n",
    "    model.eval()\n",
    "    \n",
    "    # Créer un dataloader spécifique pour la visualisation\n",
    "    test_loader = DataLoader(\n",
    "        val_loader.dataset, \n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Visualiser quelques exemples\n",
    "    print(\"\\nVisualisation des prédictions...\")\n",
    "    try:\n",
    "        num_examples_input = input(\"Nombre d'exemples à visualiser (1-5) [défaut: 1]: \")\n",
    "        num_examples = int(num_examples_input) if num_examples_input.strip() else 1\n",
    "    except ValueError:\n",
    "        print(\"Entrée invalide, utilisation de la valeur par défaut (1)\")\n",
    "        num_examples = 1\n",
    "    num_examples = max(1, min(5, num_examples))\n",
    "    \n",
    "    try:\n",
    "        conf_threshold_input = input(\"Seuil de confiance (0.1-0.9) [défaut: 0.5]: \")\n",
    "        conf_threshold = float(conf_threshold_input) if conf_threshold_input.strip() else 0.5\n",
    "    except ValueError:\n",
    "        print(\"Entrée invalide, utilisation de la valeur par défaut (0.5)\")\n",
    "        conf_threshold = 0.5\n",
    "    conf_threshold = max(0.1, min(0.9, conf_threshold))\n",
    "    \n",
    "    nms_threshold = 0.4  # Valeur standard pour NMS\n",
    "    \n",
    "    # Visualiser les prédictions\n",
    "    for i, (images, targets) in enumerate(test_loader):\n",
    "        if i >= num_examples:\n",
    "            break\n",
    "            \n",
    "        image = images[0].to(device)\n",
    "        target = targets[0]\n",
    "        \n",
    "        # Obtenir les prédictions\n",
    "        with torch.no_grad():\n",
    "            predictions = model([image])\n",
    "        \n",
    "        # Post-traitement pour obtenir les détections\n",
    "        processed_preds = []\n",
    "        for j, pred in enumerate(predictions):\n",
    "            processed = transform_predictions(pred, IMG_SIZE, ANCHORS[j], NUM_CLASSES, device)\n",
    "            processed_preds.append(processed)\n",
    "        \n",
    "        # Combiner les prédictions des trois échelles\n",
    "        detections = torch.cat(processed_preds, 1)\n",
    "        \n",
    "        # Appliquer NMS\n",
    "        output = non_max_suppression(detections, NUM_CLASSES, conf_threshold, nms_threshold)\n",
    "        \n",
    "        # Afficher l'image avec les détections\n",
    "        print(f\"\\nImage {i+1} - Visualisation des détections:\")\n",
    "        \n",
    "        # Afficher les boîtes réelles\n",
    "        img_np = image.cpu()\n",
    "        img_np = img_np * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + \\\n",
    "                 torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        img_np = torch.clamp(img_np, 0, 1)\n",
    "        \n",
    "        # Créer deux sous-figures\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
    "        \n",
    "        # Image originale avec annotations réelles\n",
    "        img_display = img_np.permute(1, 2, 0).cpu().numpy()\n",
    "        ax1.imshow(img_display)\n",
    "        ax1.set_title('Annotations réelles')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Dessiner les boîtes réelles\n",
    "        boxes = target['boxes'].cpu().numpy()\n",
    "        labels = target['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, label in zip(boxes, labels):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            class_name = VOC_CLASSES[label]\n",
    "            \n",
    "            # Mettre à l'échelle pour l'affichage\n",
    "            xmin = xmin * IMG_SIZE / image.shape[2]\n",
    "            ymin = ymin * IMG_SIZE / image.shape[1]\n",
    "            xmax = xmax * IMG_SIZE / image.shape[2]\n",
    "            ymax = ymax * IMG_SIZE / image.shape[1]\n",
    "            \n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            \n",
    "            rect = patches.Rectangle((xmin, ymin), width, height, \n",
    "                                    linewidth=2, edgecolor='green', facecolor='none')\n",
    "            ax1.add_patch(rect)\n",
    "            ax1.text(xmin, ymin, class_name, color='white', fontsize=10,\n",
    "                     bbox=dict(facecolor='green', alpha=0.7))\n",
    "        \n",
    "        # Image avec détections prédites\n",
    "        ax2.imshow(img_display)\n",
    "        ax2.set_title('Détections prédites')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # Dessiner les boîtes prédites\n",
    "        if output[0] is not None:\n",
    "            for box in output[0]:\n",
    "                x1, y1, x2, y2, conf, cls_conf, cls_pred = box\n",
    "                \n",
    "                # Mettre à l'échelle\n",
    "                x1 = x1.item()\n",
    "                y1 = y1.item()\n",
    "                x2 = x2.item()\n",
    "                y2 = y2.item()\n",
    "                \n",
    "                width = x2 - x1\n",
    "                height = y2 - y1\n",
    "                \n",
    "                class_idx = int(cls_pred.item())\n",
    "                class_name = VOC_CLASSES[class_idx + 1]  # +1 car nous avons ignoré 'background'\n",
    "                \n",
    "                rect = patches.Rectangle((x1, y1), width, height, \n",
    "                                        linewidth=2, edgecolor='red', facecolor='none')\n",
    "                ax2.add_patch(rect)\n",
    "                ax2.text(x1, y1, f\"{class_name}: {conf.item()*cls_conf.item():.2f}\", \n",
    "                         color='white', fontsize=10,\n",
    "                         bbox=dict(facecolor='red', alpha=0.7))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Pause pour voir les résultats\n",
    "        if i < num_examples - 1:\n",
    "            input(\"Appuyez sur Entrée pour voir l'exemple suivant...\")\n",
    "    \n",
    "    print(\"\\nVisualisation terminée!\")\n",
    "    print(\"Merci d'avoir utilisé le modèle YOLOv3 pour la détection d'objets!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
